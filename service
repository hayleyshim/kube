# Service
- Pod network
-- CNI에서 관리하는 포드 간 통신에 사용되는 클러스터 전체 네트워크

- Service network
-- Service discovery를 위해 kube-proxy가 관리하는 cluster wide 범위의 virtual IP

deployment.yaml
https://kubernetes.io/ko/docs/concepts/workloads/controllers/deployment/

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

services.yaml
https://kubernetes.io/ko/docs/concepts/services-networking/service/

apiVersion: v1
kind: Service
metadata:
  name: web-svc
spec:
  clusterIP: 10.96.100.100
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80


$ kubectl apply -f deployment.yaml

$ kubectl get pods | grep -i web

$ kubectl get pods -o wide

$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   250d
web-svc      ClusterIP   10.96.100.100   <none>        80/TCP    2m35s

- service가 단일 pod들의 진입점이 됨
- clusterIP를 통해서 내부 진입

$ ssh k8s-worker1

$ curl 10.96.100.100


# Service Type
- ClusterIP(default) : Pod 그룹(동일한 서비스를 지원하는 Pod 모음)의 단일 진입점(virtual IP: LB) 생성
- NodePort : ClusterIP가 생성된 후 모든 Worker Node에 외부에서 접속 가능한 포트가 예약
- LoadBalancer : 클라우드 인프라스트럭처(AWS, Azure, GCP)에 적용, LB를 자동으로 프로비저닝하는 기능 지원


# ClusterIP
- selector의 label이 동일한 pod들을 그룹으로 묶어 단일 진입점(virtual IP)을 생성
- 클러스터 내부에서만 사용가능
- service type 생략 시 default로 설정
- 10.96.0.0/12 범위에서 할당됨


# LAB : 동일한 서비스를 제공하는 pod 그룹에 clusterIP 생성하기
(deployment name : web, image:nginx, port:80, replicas:2
service name: web, type: clusterIP, port:80)

$ kubectl create deployment web --image=nginx --port=80 --replicas=2 --dry-run=client -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: web
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        resources: {}
status: {}

$ kubectl create deployment web --image=nginx --port=80 --replicas=2
$ kubectl delete deployments.apps web
$ kubectl get svc
$ kubectl get pods -o wide | grep web

$ kubectl expose deployment web --type=ClusterIP --port=80 --dry-run=client -o yaml
$ kubectl expose deployment web --type=ClusterIP --port=80
$ kubectl get svc web
$ ssh k8s-worker2
$ curl 10.99.X.X

$ kubectl get pod | grep web

# 문제
- CluterIP type의 서비스 운영
- 작업 클러스터 : k8s
- 'devops' namespace에서 운영되고 있는 eshop-order deployment의 service를 만드세요
- service name: eshop-order-svc
- type : clusterIP
- port : 80

$ kubectl get deployments.apps -n devops

$ kubectl get pod -n devops -o wide

$ kubectl describe pod --namespace devops eshop-order-5f95d86b84-fd4nc

$ kubectl get deployments.apps -n devops -o wide

$ kubectl expose deployment -n devops eshop-order --type=ClusterIP --port=80 --target-port=80 --dry-run=client -o yaml

$ kubectl expose deployment -n devops eshop-order --type=ClusterIP --port=80 --target-port=80 --name=eshop-order-svc --dry-run=client -o yaml

$ kubectl expose deployment -n devops eshop-order --type=ClusterIP --port=80 --target-port=80 --name=eshop-order-svc

$ kubectl get svc -n devops eshop-order-svc

$ kubectl describe service -n devops eshop-order-svc

$ ssh k8s-worker1

$ curl 10.101.5.245

# 문제
- Pod를 이용한 Named Service 구성
- 작업 클러스터 : k8s
- 미리 배포한 'front-end'에 기존의 nginx 컨테이너의 port '80/tcp'를 expose하는 http라는 이름을 추가합니다.
- 컨테이너 포트 http를 expose하는 front-end-svc라는 새 service를 만드시오
- 또한 준비된 node의 'nodeport'를 통해 개별 pods를 expose되도록 service를 만드시오

https://kubernetes.io/ko/docs/concepts/services-networking/service/

$ kubectl get deployments.apps

$ kubectl get deployments.apps front-end

$ kubectl get deployments.apps front-end -o yaml > front-end.yaml

front-end.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: front-end
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        name: http
        ports:
        - containerPort: 80
          name: http

---
apiVersion: v1
kind: Service
metadata:
  name: front-end-svc
spec:
  selector:
    run: nginx
  ports:
  - name: name-of-service-port
    protocol: TCP
    port: 80
    targetPort: http


$ kubectl delete deployments.apps front-end

$ kubectl apply -f front-end.yaml

$ kubectl get pods | grep front

$ kubectl get svc front-end-svc

$ kubectl edit svc front-end-svc
- service type : NodePort

$ kubectl get svc front-end-svc

$ curl k8s-worekr1:3XXXX


# NodePort
- 모든 노드를 대상으로 외부 접속 가능한 포트를 예약
- Default NodePort 범위 : 30000~32767
- ClusterIP를 생성 후 NodePort를 예약

- kubeproxy : 1. iptable rule을 만들어서 pod와 service 연결 2.nodeport listening


# 문제
- Access the service from outside the Cluster via NodePort
- 작업 클러스터 : k8s
- 'front-end' deployment의 nginx 컨테이너를 expose하는 'front-end-nodesvc'라는 새 service를 만듭니다.
- front-end로 동작중인 pod에는 node의 30200 포트로 접속되어야 합니다.

- 구성 테스트 curl k8s-worker1:30200 연결 시 nginx 홈페이지가 표시되어야 합니다.


$ kubectl config use-context k8s

$ kubectl get nodes

$ kubectl get deployments.apps front-end

$ kubectl get pods | grep -i front-end

$ kubectl describe pod front-end-9c89c87dc-hxrrn | grep -i -e port -e labels 

$ kubectl describe deployments.apps front-end | grep -i -e port -e labels
Labels:                 <none>
  Labels:  run=nginx
    Port:         80/TCP
    Host Port:    0/TCP

$ kubectl expose deployment fron-end --name front-end-svc --port=80 --target-port=80 --type NodePort --dry-run=client -o yaml > front-end-nodesvc.yaml

$ vi front-end-nodesvc.yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: front-end-svc
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30200
  selector:
    run: nginx
  type: NodePort
status:
  loadBalancer: {}

$ kubectl apply -f front-end-nodesvc.yaml

$ kubectl get svc

$ curl k8s-worker1:30200






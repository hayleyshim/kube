# Node 스케쥴링 중단 및 허용 
- 컨테이너를 포함한 Pod는 node에서 실행됨
- node는 Control-plane에 의해 관리
- 특정 node의 스케쥴링 중단(cordon) 및 허용(uncordon)

$ kubectl cordon <NODE_NAME> #스케쥴링 하지 않기
$ kubectl uncordon <NODE_NAME> #스케쥴링 하기

$ kubectl get nodes
$ kubectl get pods -o wide

$ kubectl cordon k8s-worker2 #worker2 스케쥴링 disabled

$ kubectl delete pod deploy-nginx-79488c9578-cgdqr deploy-nginx-79488c9578-d222p

$ kubectl get pods -o wide


# Node 비우기(drain)
- 특정 node에서 실행중인 pod 비우기(drain) 및 제거(delete)

$ kubectl drain <NODE_NAME> --ignore-daemonsets -force

--ignore-daemonsets DaemonSet-managed pod들은 ignore
--force=false       RC, RS, Job, DeamonSet 또는 StatefulSet에서 관리하지 않는 Pod까지 제거


$ kubectl drain k8s-worker2
node/k8s-worker2 cordoned
error: unable to drain node "k8s-worker2" due to error:cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-dvg5d, kube-system/kube-proxy-vd6zl, continuing command...
There are pending nodes to be drained:
 k8s-worker2
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-dvg5d, kube-system/kube-proxy-vd6zl


$ kubectl drain k8s-worker2 --ignore-daemonsets --force
node/k8s-worker2 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-dvg5d, kube-system/kube-proxy-vd6zl
evicting pod ingress-nginx/appjs-rc-cnk2h
evicting pod default/deploy-nginx-79488c9578-bcc8v
evicting pod default/deploy-nginx-79488c9578-qn55t
evicting pod default/eshop-payment-54cb87c674-4pnlp
evicting pod default/front-end-8dc556958-qnmvt
evicting pod default/nginx-79488c9578-djtmn
evicting pod default/webserver-5586594bbf-q8wmg
evicting pod devops/eshop-order-5f95d86b84-qpl4r
evicting pod ingress-nginx/appjs-rc-5vl8k
evicting pod ingress-nginx/appjs-rc-7l9rq
pod/webserver-5586594bbf-q8wmg evicted
pod/front-end-8dc556958-qnmvt evicted
pod/deploy-nginx-79488c9578-bcc8v evicted
I1002 02:04:58.166821   21258 request.go:665] Waited for 1.127318194s due to client-side throttling, not priority and fairness, request: GET:https://k8s-master:6443/api/v1/namespaces/default/pods/deploy-nginx-79488c9578-qn55t
pod/eshop-order-5f95d86b84-qpl4r evicted
pod/nginx-79488c9578-djtmn evicted
pod/deploy-nginx-79488c9578-qn55t evicted
pod/eshop-payment-54cb87c674-4pnlp evicted


# 문제
- 노드 비우기
- 작업 클러스터 : k8s
k8s-worker2 노드를 스케쥴링 불가능하게 설정하고 해당 노드에서 실행 중인 모든 pod를 다른 node로 reschedule 하세요


$ kubectl get nodes
$ kubectl uncordon k8s-worker2
$ kubectl get deployments.apps deploy-nginx
$ kubectl drain k8s-worker2 --ignore-daemonsets --force
$ kubectl scale deployment deploy-nginx --replicas=8 

$ kubectl drain k8s-worker2 --ignore-daemonsets --force


# Node Taint & Toleration
- node taint, pod toleration
- worker node에 taint가 설정된 경우 동일 값의 toleration이 있는 pod만 배치된다
- toleration이 있는 pod는 동일한 taint가 있는 node를 포함하여 모든 node에 배치된다.
- effect 필드 종류 
-- NodeSchedule : toleration이 맞지 않으면 배치되지않는다
-- PreferNoSchedule : toleration이 맞지 않으면 배치되지 않으나 클러스터 리소스가 부족하면 할당된다
-- NoExecute : toleration이 맞으면 동작 중인 pod를 종료

$ kubectl create deployment testdep --image=nginx --replicas=5

$ kubectl get nodes

$ kubectl describe node hk8s-master

$ kubectl describe node hk8s-w1 | grep -i taint

$ kubectl describe node hk8s-w2 | grep -i taint

$ kubectl describe node hk8s-m | grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule


$ kubectl create deployment testdep --image=nginx --replicas=5 --dry-run=client -o yaml > testdep.yaml

testdep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: testdep
spec:
  replicas: 5
  selector:
    matchLabels:
      app: testdep
  strategy: {}
  template:
    metadata:
      labels:
        app: testdep
    spec:
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Equal"
        effect: "NoSchedule"
      containers:
      - image: nginx
        name: nginx

$ kubectl describe node hk8s-m | grep -i taint
$ kubectl describe node hk8s-w1 | grep -i taint


# 문제
-Ready 노드 확인하기
- 작업 클러스터 : hk8s
- Ready 상태(NoSchedule로 Taint된 node는 제외)인 node를 찾아 그 수를 /var/CKA2022/notaint_ready_node에 기록하세요


$ kubectl config use-context hk8s

$ kubectl get nodes

$ kubectl describe node hk8s-m | grep -i -e taint -e noschedule 
$ kubectl describe node hk8s-w1 | grep -i -e taint -e noschedule 
$ kubectl describe node hk8s-w2 | grep -i -e taint -e noschedule 

ready : 3
noschedule : 1
3-1 ==> 2

$ echo "2" > /var/CKA2022/notaint_ready_node
$ cat /var/CKA2022/notaint_ready_node
2
